"""å¯¹è¯å¤„ç†å¼•æ“"""
import asyncio
import logging
from datetime import datetime
from typing import Optional, List, Dict, Any
from openai import AsyncOpenAI
from ..config import settings
from ..clients import CogneeClientWrapper, MemobaseClientWrapper, Mem0ClientWrapper
from ..services import KnowledgeService, ProfileService, MemoryService
from ..prompts.templates import build_conversation_prompt, get_system_prompt

logger = logging.getLogger(__name__)


class ConversationEngine:
    """å¯¹è¯å¤„ç†å¼•æ“"""
    
    def __init__(
        self,
        openai_client: AsyncOpenAI,
        cognee_client: CogneeClientWrapper,
        memobase_client: MemobaseClientWrapper,
        mem0_client: Mem0ClientWrapper
    ):
        self.openai = openai_client
        self.knowledge_service = KnowledgeService(cognee_client)
        self.profile_service = ProfileService(memobase_client)
        self.memory_service = MemoryService(mem0_client)
    
    async def process_message(
        self,
        user_id: str,
        session_id: str,
        message: str,
        dataset_names: Optional[List[str]] = None,
        role: str = "default"
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼Œç”Ÿæˆå“åº”
        
        Args:
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            message: ç”¨æˆ·æ¶ˆæ¯
            dataset_names: çŸ¥è¯†åº“æ•°æ®é›†åç§°åˆ—è¡¨
        
        Returns:
            åŒ…å«å“åº”å’Œä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        # æ­¥éª¤ 1-3ï¼šå¹¶å‘è·å–ä¸Šä¸‹æ–‡ï¼ˆğŸš€ å·²ä¼˜åŒ–æ€§èƒ½ï¼‰
        import time
        start_time = time.time()
        retrieval_start = time.time()
        
        user_profile, session_memories, knowledge_results = await asyncio.gather(
            self.profile_service.get_user_profile(user_id=user_id, max_token_size=300),  # ğŸš€ å‡å°‘token
            self.memory_service.get_conversation_context(
                user_id=user_id,
                session_id=session_id,
                query=message
            ),
            self.knowledge_service.search_knowledge(
                query=message,
                dataset_names=dataset_names or [],
                top_k=2  # ğŸš€ ä»5å‡å°‘åˆ°2ï¼Œæ˜¾è‘—åŠ å¿«æ£€ç´¢é€Ÿåº¦
            ),
            return_exceptions=True
        )
        
        retrieval_time = time.time() - retrieval_start
        logger.info(f"âš¡ å¹¶è¡Œæ£€ç´¢è€—æ—¶: {retrieval_time:.2f}ç§’")
        
        # å¤„ç†å¼‚å¸¸å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
        if isinstance(user_profile, Exception):
            logger.warning(f"Failed to get user profile (will use empty profile): {user_profile}")
            user_profile = {}
        else:
            logger.info(f"Retrieved user profile: {len(user_profile)} fields")
        
        if isinstance(session_memories, Exception):
            logger.warning(f"Failed to get session memories (will use empty memories): {session_memories}")
            session_memories = []
        else:
            logger.info(f"Retrieved {len(session_memories)} session memories")
        
        if isinstance(knowledge_results, Exception):
            # å¦‚æœæ˜¯æ•°æ®é›†ä¸å­˜åœ¨çš„é”™è¯¯ï¼Œç»™å‡ºå‹å¥½æç¤º
            error_msg = str(knowledge_results)
            if "DatasetNotFoundError" in error_msg or "No datasets found" in error_msg:
                logger.warning(f"Dataset not found (will continue without knowledge): {error_msg}")
            else:
                logger.warning(f"Failed to get knowledge (will continue without knowledge): {error_msg}")
            knowledge_results = []
        else:
            logger.info(f"Retrieved {len(knowledge_results)} knowledge results")
        
        # æ­¥éª¤ 4ï¼šæ„å»º Prompt
        prompt = build_conversation_prompt(
            user_profile=user_profile,
            session_memories=session_memories,
            knowledge=knowledge_results,
            user_message=message
        )
        
        # æ­¥éª¤ 5ï¼šè°ƒç”¨ OpenAI API
        llm_start = time.time()
        try:
            response = await self.openai.chat.completions.create(
                model=settings.openai_model,
                messages=[
                    {"role": "system", "content": get_system_prompt(role)},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=800  # ğŸš€ é™åˆ¶å›å¤é•¿åº¦ï¼ŒåŠ å¿«ç”Ÿæˆ
            )
            ai_response = response.choices[0].message.content
            llm_time = time.time() - llm_start
            logger.info(f"âš¡ LLMç”Ÿæˆè€—æ—¶: {llm_time:.2f}ç§’")
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            ai_response = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
            llm_time = time.time() - llm_start
        
        # æ­¥éª¤ 6-7ï¼šå¼‚æ­¥ä¿å­˜ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        asyncio.create_task(
            self._save_conversation_async(
                user_id=user_id,
                session_id=session_id,
                user_message=message,
                ai_response=ai_response,
                dataset_names=dataset_names
            )
        )
        
        # æ€»è€—æ—¶
        total_time = time.time() - start_time
        logger.info(f"ğŸ¯ å¯¹è¯æ€»è€—æ—¶: {total_time:.2f}ç§’ (æ£€ç´¢: {retrieval_time:.2f}s + LLM: {llm_time:.2f}s)")
        
        # è¿”å›å“åº”å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨äºæµ‹è¯•å’Œè°ƒè¯•ï¼‰
        # ç¡®ä¿å³ä½¿æ•°æ®ä¸ºç©ºä¹Ÿè¿”å›æœ‰æ„ä¹‰çš„ä¿¡æ¯
        context = {
            "user_profile": user_profile if user_profile else {},
            "user_profile_status": "å·²åŠ è½½" if user_profile else "æš‚æ— ï¼ˆé¦–æ¬¡å¯¹è¯æˆ–æ–°ç”¨æˆ·ï¼‰",
            "session_memories_count": len(session_memories),
            "session_memories_status": f"å·²åŠ è½½ {len(session_memories)} æ¡è®°å¿†" if session_memories else "æš‚æ— ï¼ˆé¦–æ¬¡å¯¹è¯æˆ–æ–°ä¼šè¯ï¼‰",
            "knowledge_count": len(knowledge_results),
            "knowledge_status": f"å·²æ£€ç´¢åˆ° {len(knowledge_results)} æ¡çŸ¥è¯†" if knowledge_results else "æš‚æ— ï¼ˆæœªæŒ‡å®šçŸ¥è¯†åº“æˆ–çŸ¥è¯†åº“ä¸ºç©ºï¼‰",
            "session_memories": session_memories[:5] if session_memories else [],  # åªè¿”å›å‰5æ¡
            "knowledge": knowledge_results[:3] if knowledge_results else [],  # åªè¿”å›å‰3æ¡
        }
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨æœ‰é”™è¯¯æ—¶ï¼‰
        if any(isinstance(x, Exception) for x in [user_profile, session_memories, knowledge_results]):
            context["debug"] = {
                "profile_error": str(user_profile) if isinstance(user_profile, Exception) else None,
                "memories_error": str(session_memories) if isinstance(session_memories, Exception) else None,
                "knowledge_error": str(knowledge_results) if isinstance(knowledge_results, Exception) else None
            }
        
        return {
            "response": ai_response,
            "context": context
        }
    
    async def _save_conversation_async(
        self,
        user_id: str,
        session_id: str,
        user_message: str,
        ai_response: str,
        dataset_names: Optional[List[str]] = None
    ) -> None:
        """å¼‚æ­¥ä¿å­˜ä¼šè¯è®°å¿†å’Œæ›´æ–°ç”¨æˆ·ç”»åƒ"""
        try:
            await asyncio.gather(
                self.memory_service.save_conversation(
                    user_id=user_id,
                    session_id=session_id,
                    messages=[
                        {"role": "user", "content": user_message},
                        {"role": "assistant", "content": ai_response}
                    ],
                    metadata={
                        "dataset_names": dataset_names,
                        "timestamp": datetime.now().isoformat()
                    }
                ),
                self.profile_service.extract_and_update_profile(
                    user_id=user_id,
                    messages=[
                        {"role": "user", "content": user_message},
                        {"role": "assistant", "content": ai_response}
                    ]
                ),
                return_exceptions=True
            )
        except Exception as e:
            logger.error(f"Failed to save conversation: {e}", exc_info=True)

